{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173af00e-b7ce-4adb-9b28-5fc052126bb3",
   "metadata": {},
   "source": [
    "# Welcome to the autodiff30 tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271021f-4c5b-4181-8752-fe01deb1ef57",
   "metadata": {},
   "source": [
    "This tutorial will show you how to install and then use autodiff30. This is a fairly long notebook, the table of contents in the left hand menu may help jump between sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524a978-d1e3-4a4f-abe5-127e2655b1f7",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7df68-dda7-483b-bb91-c4006ea07d62",
   "metadata": {},
   "source": [
    "It is recommended to work in a virtual environment to avoid and potential dependency conflicts, e.g. in Anaconda Prompt you can run:\n",
    "```\n",
    "conda create -n autodiff30_testing -y python=3.10.4 numpy jupyterlab\n",
    "conda activate autodiff30_testing\n",
    "```\n",
    "Once in the environment install the package from PyPI:\n",
    "```\n",
    "python -m pip install autodiff30\n",
    "```\n",
    "Then to run this tutorial copy this jupyter file into the folder in which you would like to run it, navigate there in Anaconda Prompt and run ```jupyterlab```. This example is just for conda but the same can be achieved using any other distribution and a package like pyenv for environment management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19add044-e80c-46ed-b836-96f3d6565e16",
   "metadata": {},
   "source": [
    "## Basic package usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d71d5-1d7b-4239-961f-46b4f952b976",
   "metadata": {},
   "source": [
    "Before using the package it needs to be imported into the script, we recommend the alias ad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f85e4e74-1b88-4917-bac6-ee9b4b7a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autodiff30 as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64a085-773a-4cfd-ba50-1bc30150ab40",
   "metadata": {},
   "source": [
    "The key data structure the package provides is a decorator called ```adfunction```. This wraps a user-defined function and allows the gradient of that function be be computed by accessing the ```grad``` method of the resulting data structure (which is called an ```adstruc```, not that this is neccessary for you to know). We can see an example below for an extremely simple function, which just ```square```s an input. Clearly the derivative of this is just double the input.\n",
    "\n",
    "Arithmetic within user-defined functions can be written as normal, but please see later sections for how more complex functions need to be written (e.g. using exponentials or trigonometry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "230f0f40-5bed-4760-bfc2-0193fe6f45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your function with the decorator\n",
    "@ad.adfunction\n",
    "def square(x):\n",
    "    \"\"\"\n",
    "    x is a numeric type, int or float\n",
    "    square returns the square of the numeric input\n",
    "    \"\"\"\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd5953-ff2c-4dd6-aa5f-9c7c6e6b37fc",
   "metadata": {},
   "source": [
    "We can see the type of square has now changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55d90b23-a3c5-460d-b31d-a479f497cc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autodiff30.ad.adstruc"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a317-b4f1-4b5a-96d6-fb36a78843d3",
   "metadata": {},
   "source": [
    "We can still call the function as we would previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26404577-8ef3-46af-900d-e74b8eb4cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_input = 3\n",
    "square(numeric_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeba3c6-b7cf-4fc9-8104-8585999f35d8",
   "metadata": {},
   "source": [
    "But we now have access to a new method, grad, which uses automatic differentiation to find the gradient of *square* at *numeric_input*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1b64970-c536-42d2-abcc-abfbc9289a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square.grad(numeric_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566eabf-271b-47bc-9150-1ec2a5a24a4e",
   "metadata": {},
   "source": [
    "It is as simple as that! Next we will show some examples with more complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558fc00-19ec-42d0-9f91-7dfbb402f256",
   "metadata": {},
   "source": [
    "## Advanced package usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db204b7-adfc-45d4-9ed3-dab3e7fb09dd",
   "metadata": {},
   "source": [
    "### Complex operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4ae10-842e-4136-9a83-5cf1556cb5d7",
   "metadata": {},
   "source": [
    "First, we will still consider scalar functions, but we will involve more complex operators. As noted previously, arithmetic, including any of the operators listed below, can be written as in vanilla python. Supported operations include multiplication, addition, subtraction, divison, powers and comparison.\n",
    "\n",
    "autodiff30 supports a number of higher level functions. **These must come from the ad package, numpy equivalents will not work**. For the full list of available functions please see functions.py in the source code, but in brief available functions are:\n",
    " - Trigonometric (sin, cos, tan, arcsin, arccos, arctan)\n",
    " - Exponential, logarithmic and logistic\n",
    " - Hyperbolic (sinh, cosh, tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f54fa-b6a8-428b-95f4-917cdd6cf175",
   "metadata": {},
   "source": [
    "##### A hard-to-differentiate arithmetic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ec71be9-01b8-4c99-ae3a-1a41f9f881da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ad.adfunction\n",
    "def hard_to_diff(x):\n",
    "    \"\"\"\n",
    "    x is a numeric type, int or float\n",
    "    hard_to_diff returns a float\n",
    "    \"\"\"\n",
    "    return x*((3*(x**2)) - (4*x) + (5/x))**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ca732e1-e7c0-4962-96c5-ff3d7aab15e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 64.0\n",
      "Gradient value: -80.0\n"
     ]
    }
   ],
   "source": [
    "evaluated_at = 1\n",
    "print(\"Function value: {}\".format(hard_to_diff(evaluated_at)))\n",
    "print(\"Gradient value: {}\".format(hard_to_diff.grad(evaluated_at)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159143f2-23cc-4e6b-b149-f9ea548ce83c",
   "metadata": {},
   "source": [
    "This result can be confirmed at https://www.wolframalpha.com/input?i=differentiate+x%283x%5E2+-+4x+%2B+5%2Fx%29%5E3+at+x%3D1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f36f64-2f67-48f1-b70e-105c673b2438",
   "metadata": {},
   "source": [
    "##### A simple trigonometric function\n",
    "Note the use of ad.sin and ad.cos in the below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e9d0a48-a73a-42b0-9bc5-7ef7ffd2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ad.adfunction\n",
    "def trig(x):\n",
    "    \"\"\"\n",
    "    x is a numeric type, int or float\n",
    "    trig returns a float\n",
    "    \"\"\"\n",
    "    return (ad.sin(x)**2) + (ad.cos(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42723509-4052-4f46-910e-4977b02db178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 1.0\n",
      "Gradient value: -0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from numpy import pi\n",
    "evaluated_at = pi/2\n",
    "print(\"Function value: {}\".format(trig(evaluated_at)))\n",
    "print(\"Gradient value: {}\".format(trig.grad(evaluated_at)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7601cf-fa3c-403e-826b-d58a24ce6d17",
   "metadata": {},
   "source": [
    "This result can be confirmed at https://www.wolframalpha.com/input?i=+differentiate+sin%28x%29%5E2+%2B+cos%28x%29+at+x%3Dpi%2F2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb5c2e-9921-447f-9d2d-da0144fa4455",
   "metadata": {},
   "source": [
    "##### A function with multiple inputs\n",
    "Autodiff30 handles functions with multiple inputs and multiple outputs using Python built-in lists. For example the below function has 3 inputs and outputs a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "756b9050-1a78-44ce-ae58-c6c25ef0e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ad.adfunction\n",
    "def R3_to_R(x):\n",
    "    \"\"\"\n",
    "    x is a list of numeric types, int or float\n",
    "    R3_to_R returns an int or float\n",
    "    \"\"\"\n",
    "    return (x[0]*x[2]) + (x[1]**2) + (x[2]**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3e090e1-34a4-49e9-96e3-bc68d4770de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 80\n",
      "Gradient value: [4, 4, 51]\n"
     ]
    }
   ],
   "source": [
    "evaluated_at = [3,2,4]\n",
    "print(\"Function value: {}\".format(R3_to_R(evaluated_at)))\n",
    "print(\"Gradient value: {}\".format(R3_to_R.grad(evaluated_at)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7237d33-6700-4281-98be-57fe17b7cc3a",
   "metadata": {},
   "source": [
    "Notice here how the output is now a list, as the gradient of the function is taken with respect to the three different input directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b2e16-fa89-4f2b-bb16-77e013d914cc",
   "metadata": {},
   "source": [
    "##### A function with multiple inputs and outputs\n",
    "The below function has 2 inputs and 2 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbee0be1-5593-4052-99dd-c892ca5df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ad.adfunction\n",
    "def R2_to_R2(x):\n",
    "    \"\"\"\n",
    "    x is a list of numeric types, int or float\n",
    "    R2_to_R2 returns a list of floats\n",
    "    \"\"\"\n",
    "    return [(x[0]**2), (x[0]**2)*(x[1]**2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b6b8a7f-076b-46b5-a773-e6a34a4efc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: [4, 36]\n",
      "Gradient value: [[4, 0], [36, 24]]\n"
     ]
    }
   ],
   "source": [
    "evaluated_at = [2,3]\n",
    "print(\"Function value: {}\".format(R2_to_R2(evaluated_at)))\n",
    "print(\"Gradient value: {}\".format(R2_to_R2.grad(evaluated_at)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5cd6-9f86-4d6c-9ecc-26b1610f9014",
   "metadata": {},
   "source": [
    "Now we see that the output is a 2x2 matrix, in list form. This is the jacobian matrix, with each element representing the derivative of one of the oupts with respect to one of the inputs. Specifically, if we let $u$ = R2_to_R2() then the jacobian here is:\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbf{u} =\n",
    "\\begin{bmatrix}\n",
    "  \\frac{\\partial u_0}{\\partial x_0} & \n",
    "    \\frac{\\partial u_0}{\\partial x_1} \\\\[1ex] % <-- 1ex more space between rows of matrix\n",
    "  \\frac{\\partial u_1}{\\partial x_0} & \n",
    "    \\frac{\\partial u_1}{\\partial x_1} \\\\[1ex]\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e021623-59a0-4dc1-b3e7-0794d7cdf6ec",
   "metadata": {},
   "source": [
    "### An applied example\n",
    "\n",
    "Here we will imagine we want to find the roots of a polynomial $f(x) = 6x^5-5x^4-4x^3+3x^2$ using [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method). This example is taken from [this](https://danielhomola.com/learning/newtons-method-with-10-lines-of-python/) website. The polynomial can be factored to show that the roots are x = 0, 1, ~ -0.79 and ~ 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0462ac6-f4ab-4871-96ae-84e8e2cf6eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root is at:  0\n",
      "f(x) at root is:  0\n",
      "Root is at:  0.6286680781673307\n",
      "f(x) at root is:  -1.3785387997788945e-06\n",
      "Root is at:  1\n",
      "f(x) at root is:  0\n"
     ]
    }
   ],
   "source": [
    "# Define the polynomial\n",
    "@ad.adfunction\n",
    "def f(x):\n",
    "    return (6*x**5)-(5*x**4)-(4*x**3)+(3*x**2)\n",
    "          \n",
    "# Define a function that tells us how far from a root (f(x)=0) we are\n",
    "def dx(f, x):\n",
    "    return abs(0-f(x))\n",
    "                   \n",
    "def newtons_method(f, x0, e):\n",
    "    # Get initial distance from root\n",
    "    delta = dx(f, x0)\n",
    "    while delta >= e:  # While not close enough to a root\n",
    "        # Update the guess for x, using the grad method of f\n",
    "        x0 = x0 - f(x0)/f.grad(x0)\n",
    "        # Get new distance from root\n",
    "        delta = dx(f, x0)\n",
    "    print('Root is at: ', x0)\n",
    "    print('f(x) at root is: ', f(x0))\n",
    "                   \n",
    "# Try Newton's method for a few different starting guesses\n",
    "x0s = [0, .5, 1]\n",
    "for x0 in x0s:\n",
    "    newtons_method(f, x0, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e9b62-10b8-4f34-bb37-a3694c6774cc",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453a4ff-ac4c-4366-b718-a8e1eaa75d49",
   "metadata": {},
   "source": [
    "autodiff30 includes functions that use the underlying automatic differentiation capabilites to do optimization. Please see the background section of the documentation for a more complete introduction to these functions, only their application is considered here. Here we will use the optimization functions to find the minimum of a $R^2->R$ function, which can be visualized [here](https://www.wolframalpha.com/input?i=plot+x**2+%2B+y**2+-+10sin%28x%29*cos%28y%29+from+-10+to+10). We see that as before, we need to set our function to minimize using the ```adfunction``` decorator and ```ad.sin```, ```ad.cos``` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dc44e18-95e4-4e34-bab2-5705803856a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Solution for minimizing f with x0 = [1.0, 1.0] and algorithm GD is [4.50183627e-01 3.74356753e-08]\n",
      "\n",
      " Solution for minimizing f with x0 = [1.0, 1.0] and algorithm Adam is [ 4.50183526e-01 -6.36105660e-08]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@ad.adfunction\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    x is a list of numeric types, int or float\n",
    "    f returns a float\n",
    "    \"\"\"\n",
    "    return x[0]**2 + x[1]**2 - ad.sin(x[0])*ad.cos(x[1])\n",
    "\n",
    "# Starting point\n",
    "x0 = [1., 1.]\n",
    "\n",
    "# Run optimization\n",
    "res1 = ad.GD(f, x0)\n",
    "res2 = ad.Adam(f, x0)\n",
    "\n",
    "# The functions return the cooridnates of the minima\n",
    "print(f\" Solution for minimizing f with x0 = {x0} and algorithm GD is {res1}\\n\")\n",
    "print(f\" Solution for minimizing f with x0 = {x0} and algorithm Adam is {res2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34717693-d7fe-473b-a1cf-d531f3a6f121",
   "metadata": {},
   "source": [
    "Note there are many input parameters to these optimizers that may need exploring in order to get certain functions to converge (e.g. the learning rate and maximum number of iterations). See the docstring for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3626-496f-4482-9f95-90119959b5d4",
   "metadata": {},
   "source": [
    "There are further examples in the docs/examples folder. Happy differentiating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
